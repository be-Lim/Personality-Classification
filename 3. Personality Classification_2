/* Coded in Google Colab, Used T4 GPU */

# **Library**

import os
os.chdir("/content/drive/MyDrive/GRIT")
%load_ext autoreload
%autoreload 2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV
!pip install bayesian-optimization
from bayes_opt import BayesianOptimization
from sklearn.ensemble import VotingClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, accuracy_score

# **Data Preprocessing**

#데이터 불러오기
df = pd.read_csv('/content/drive/MyDrive/GRIT/MBTI/16P.csv', encoding='ISO-8859-1')
print(df.shape)
print(df.head())

#결측치 확인
print(df.isnull().sum())

mbti_mapping = {
    'ESTJ': 0, 'ENTJ': 1, 'ESFJ': 2, 'ENFJ': 3,
    'ISTJ': 4, 'ISFJ': 5, 'INTJ': 6, 'INFJ': 7,
    'ESTP': 8, 'ESFP': 9, 'ENTP': 10, 'ENFP': 11,
    'ISTP': 12, 'ISFP': 13, 'INTP': 14, 'INFP': 15
}
mbti_inverse_mapping = {v: k for k, v in mbti_mapping.items()}

def encode_mbti_column(df, column_name):
    df[column_name + '_encoded'] = df[column_name].map(mbti_mapping)
    return df
def decode_mbti_column(df, column_name_encoded):
    df[column_name_encoded.replace('_encoded', '') + '_decoded'] = df[column_name_encoded].map(mbti_inverse_mapping)
    return df

#'Personality' 인코딩
df_encoded = encode_mbti_column(df, 'Personality')
df_encoded = df_encoded.drop(['Personality'], axis=1)
df_encoded.head()

df_dropped = df_encoded.drop(['Response Id', 'Personality_encoded'], axis=1)
X = df_dropped.values.reshape(-1, 60)
print(X)

mbti_labels = df['Personality']

# **Sentence Embedding**

df_dropped = df_encoded.drop(['Response Id'], axis=1)
questions = pd.Series(df_dropped.columns[:-1])
print(questions)

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
question_embeddings = model.encode(questions)

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering

similarity_matrix = cosine_similarity(question_embeddings)

clustering = AgglomerativeClustering(n_clusters=8, distance_threshold=None, metric='cosine', linkage='average')
clusters = clustering.fit_predict(question_embeddings)

print(clusters)

selected_question_indices = []
seen_clusters = set()

for idx, cluster_id in enumerate(clusters):
    if cluster_id not in seen_clusters:
        selected_question_indices.append(idx)
        seen_clusters.add(cluster_id)

filtered_questions = [questions[i] for i in selected_question_indices]
print(filtered_questions)

#최종 선택된 데이터
final_features = final_questions + ['Personality_encoded']
df_selected = df_encoded[final_features]
#데이터 분할
X = df_selected.drop('Personality_encoded', axis=1).values
y = df_selected['Personality_encoded'].astype(int).values
#표준 정규화
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled)
print(y)

# **Sorting Questions**

def binary_targets(mbti_list, index, target_char):
  return np.array([1 if mbti[index] == target_char else 0 for mbti in mbti_list])

!pip install lightgbm --quiet
from lightgbm import LGBMClassifier
def importance_lgbm(X, y):
  model = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
  model.fit(X, y)
  y_pred = model.predict(X)

  print("Accuracy:", accuracy_score(y, y_pred))

  feature_importances = model.feature_importances_

  return feature_importances

top_k = 8

# E/I
y_EI = binary_targets(mbti_labels, 0, 'E')
scores_EI = importance_lgbm(X, y_EI)
top_EI = np.argsort(scores_EI)[-top_k:]
questions = pd.Series(df_dropped.columns)
print(top_EI)
print(questions[top_EI])

# N/S
y_NS = binary_targets(mbti_labels, 1, 'N')
scores_NS = importance_lgbm(X, y_NS)
top_NS = np.argsort(scores_NS)[-top_k:]
print(top_NS)
print(questions[top_NS])

# F/T
y_FT = binary_targets(mbti_labels, 2, 'F')
scores_FT = importance_lgbm(X, y_FT)
top_FT = np.argsort(scores_FT)[-top_k:]
print(top_FT)
print(questions[top_FT])

# J/P
y_JP = binary_targets(mbti_labels, 3, 'J')
scores_JP = importance_lgbm(X, y_JP)
top_JP = np.argsort(scores_JP)[-top_k:]
print(top_JP)
print(questions[top_JP])

questions_index = list(set().union(top_EI, top_FT, top_NS, top_JP))
print(questions[questions_index])
print(len(questions_index))

#최종 선택된 데이터
final_features = list(questions[questions_index]) + ['Personality_encoded']
print(final_features)

df_selected = df_encoded[final_features]
X = df_selected.drop('Personality_encoded', axis=1).values
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
print(len(X_scaled))

y = df_selected['Personality_encoded'].values
print(len(y))

# **Classification Model - KNN**

#모델 훈련 및 초기 성능 측정
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

knn_scores = cross_val_score(knn, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"KNN: 평균 정확도 = {knn_scores.mean():.4f}, 표준편차 = {knn_scores.std():.4f}")

# 하이퍼파라미터 최적화 함수 정의
def knn_cv(n_neighbors):
    model = KNeighborsClassifier(n_neighbors=int(n_neighbors))
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.4, shuffle=True, stratify=y, random_state=42)

# 파라미터 탐색 범위 설정
param_bounds = {
    'n_neighbors': (1, 30)
}

# 베이지안 최적화 실행
optimizer = BayesianOptimization(
    f=knn_cv,
    pbounds=param_bounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=3, n_iter=10)

# 최적 파라미터 출력
best_params = optimizer.max['params']
best_params['n_neighbors'] = int(best_params['n_neighbors'])

print("\n Best Parameters:")
print(best_params)

#최적화 모델 성능 측정
best_knn = KNeighborsClassifier(n_neighbors=25)
best_knn.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

best_knn_scores = cross_val_score(best_knn, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"best_KNN: 평균 정확도 = {best_knn_scores.mean():.4f}, 표준편차 = {best_knn_scores.std():.4f}")

# **Classification Model - ANN**

# 클래스 수 확인 및 원핫 인코딩
num_classes = len(np.unique(y))
y_encoded = to_categorical(y, num_classes)
# 학습/테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, shuffle=True, random_state=42)
print(X_test)

#모델 생성
ann_model = Sequential([
    Dense(256, activation='relu', input_shape=(X.shape[1],)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),

    Dense(num_classes, activation='softmax')
])

#모델 컴파일 및 학습
ann_model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

ann_model.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=64, verbose=1)

#모델 평가
y_pred = ann_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred_classes)
print("Accuracy:", accuracy)

# **Model Tuning - ANN**

X_sample, _, y_sample, _ = train_test_split(X_scaled, y_encoded, train_size=0.2, shuffle=True, random_state=42)

#모델 생성 함수
def build_model(dropout_rate, learning_rate):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(X.shape[1],)),
        BatchNormalization(),
        Dropout(dropout_rate),

        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),

        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),

        Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

#Bayesian Optimization 수행 함수
def model_cv(dropout_rate, learning_rate):
    model = build_model(dropout_rate=dropout_rate, learning_rate=learning_rate)
    history = model.fit(X_sample, y_sample, validation_split=0.1, epochs=30, batch_size=64, verbose=0)
    val_acc = max(history.history['val_accuracy'])
    return val_acc

#탐색 범위 정의 및 최적화 수행
pbounds = {
    'dropout_rate': (0.1, 0.5),
    'learning_rate': (1e-4, 1e-2)
}

optimizer = BayesianOptimization(f=model_cv, pbounds=pbounds, random_state=42)
optimizer.maximize(init_points=5, n_iter=10)

#최적 하이퍼파라미터 출력
print("\n Best Parameters:")
print(optimizer.max)

#모델 생성
def best_ann(input_dim, num_classes):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,)),
        BatchNormalization(),
        Dropout(0.1635),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.1635),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=0.00035),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 학습
best_ann = best_ann(X_train.shape[1], num_classes)
best_ann.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=64, verbose=1)

#최적화 모델 평가
y_pred = best_ann.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred_classes)
print("best_Accuracy:", accuracy)

# **Ensemble(KNN+ANN)**

def ensemble_model(knn, ann, X):
  knn_probs = knn.predict_proba(X)
  ann_probs = ann.predict(X)

  avg_probs =(knn_probs + ann_probs) / 2

  return avg_probs

y_pred_classes = np.argmax(ensemble_model(best_knn, best_ann, X_scaled), axis=1)

y_true = np.argmax(y_encoded, axis=1)
ensemble_accuracy = accuracy_score(y_true, y_pred_classes)
print("best_Accuracy:", ensemble_accuracy)

for i, prob in enumerate(ensemble_model(best_knn, best_ann, X_scaled)[0]):
    print(f"{mbti_inverse_mapping[i]}: {prob:.4f}")

# **Test**

#(-3 ~ 3) = (전혀 그렇지 않다 ~ 매우 그렇다)
from google.colab import files
flattened_questions = list(questions[questions_index])
questions_with_blank_answers = np.column_stack((flattened_questions, [''] * len(flattened_questions)))
data = pd.DataFrame(questions_with_blank_answers, columns=["question", "answer"])
data.to_csv("/content/drive/MyDrive/GRIT/MBTI/test_question.csv", index=False)
files.download("/content/drive/MyDrive/GRIT/MBTI/test_question.csv")

test_answer = pd.read_csv('/content/drive/MyDrive/GRIT/MBTI/test_answer.csv', encoding='ISO-8859-1')
data = test_answer.iloc[:, 1]
data = data.values.reshape(1, -1)
print(data)

data_scaled = scaler.fit_transform(data.T)
print(data_scaled.T)
input_data = data_scaled.T
prediction = ensemble_model(best_knn, best_ann, input_data)
prediction_classes = np.argmax(prediction, axis=1)
print(prediction_classes)
print("Predicted_MBTI:", mbti_inverse_mapping[prediction_classes[0]])
for i, prob in enumerate(prediction[0]):
    print(f"{mbti_inverse_mapping[i]}: {prob:.4f}")
