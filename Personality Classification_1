import os
os.chdir("/content/drive/MyDrive/GRIT")
%load_ext autoreload
%autoreload 2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
import lightgbm as lgb
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV
!pip install bayesian-optimization
from bayes_opt import BayesianOptimization
from sklearn.ensemble import VotingClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, accuracy_score

# **EDA**

#데이터 불러오기
df = pd.read_csv('/content/drive/MyDrive/GRIT/MBTI/16P.csv', encoding='ISO-8859-1')
print(df.shape)
print(df.head())

#결측치 확인
print(df.isnull().sum())

mbti_mapping = {
    'ESTJ': 0, 'ENTJ': 1, 'ESFJ': 2, 'ENFJ': 3,
    'ISTJ': 4, 'ISFJ': 5, 'INTJ': 6, 'INFJ': 7,
    'ESTP': 8, 'ESFP': 9, 'ENTP': 10, 'ENFP': 11,
    'ISTP': 12, 'ISFP': 13, 'INTP': 14, 'INFP': 15
}
mbti_inverse_mapping = {v: k for k, v in mbti_mapping.items()}

def encode_mbti_column(df, column_name):
    df = df.copy()
    df[column_name + '_encoded'] = df[column_name].map(mbti_mapping)
    return df
def decode_mbti_column(df, column_name_encoded):
    df = df.copy()
    df[column_name_encoded.replace('_encoded', '') + '_decoded'] = df[column_name_encoded].map(mbti_inverse_mapping)
    return df

#'Personality' 인코딩
df_encoded = encode_mbti_column(df, 'Personality')
df_encoded = df_encoded.drop(['Personality'], axis=1)
df_encoded.head()

#데이터 분할
X = df_encoded.drop('Personality_encoded', axis=1).values
y = df_encoded['Personality_encoded'].astype(int).values
#표준 정규화
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled)

# **Classification Model - KNN**

#모델 훈련 및 초기 성능 측정
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

knn_scores = cross_val_score(knn, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"{'KNN':12s}: 평균 정확도 = {knn_scores.mean():.4f}, 표준편차 = {knn_scores.std():.4f}")

# **Classification Model - LightGBM**

#모델 훈련 및 초기 성능 측정
lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(np.unique(y)), random_state=42)
lgb_model.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

lgb_scores = cross_val_score(lgb_model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"{'LightGBM':12s}: 평균 정확도 = {lgb_scores.mean():.4f}, 표준편차 = {lgb_scores.std():.4f}")

# **Classification Model - XGBoost**

#모델 훈련 및 초기 성능 측정
X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)

xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    n_estimators=300,
    learning_rate=0.1,
    max_depth=6,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)
xgb_model.fit(X_sample, y_sample)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

xgb_scores = cross_val_score(xgb_model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"{'XGBoost':12s}: 평균 정확도 = {xgb_scores.mean():.4f}, 표준편차 = {xgb_scores.std():.4f}")

# **Classification Model - ANN**

# 클래스 수 확인 및 원핫 인코딩
num_classes = len(np.unique(y))
y_encoded = to_categorical(y, num_classes)
# 학습/테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

#모델 생성
model = Sequential([
    Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=(X.shape[1],)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(128, activation='relu', kernel_initializer='he_normal'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(64, activation='relu', kernel_initializer='he_normal'),
    BatchNormalization(),
    Dropout(0.2),

    Dense(num_classes, activation='softmax')  # 다중 클래스 분류
])

#모델 컴파일 및 학습
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train,
                    validation_split=0.1,
                    epochs=50,
                    batch_size=32,
                    verbose=1)

#모델 평가
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred_classes)
print("Accuracy:", accuracy)

# **Model Tuning - KNN**

# 하이퍼파라미터 최적화 함수 정의
def knn_cv(n_neighbors):
    model = KNeighborsClassifier(n_neighbors=int(n_neighbors))
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)

# 파라미터 탐색 범위 설정
param_bounds = {
    'n_neighbors': (1, 30)  # KNN에서는 일반적으로 1~30 사이가 적절
}

# 베이지안 최적화 실행
optimizer = BayesianOptimization(
    f=knn_cv,
    pbounds=param_bounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=3, n_iter=10)

# 최적 파라미터 출력
best_params = optimizer.max['params']
best_params['n_neighbors'] = int(best_params['n_neighbors'])

print("\n Best Parameters:")
print(best_params)

# **Model Tuning - ANN**

!pip install scikeras
from scikeras.wrappers import KerasClassifier

X_sample, _, y_sample, _ = train_test_split(X_scaled, y_encoded, train_size=0.2, random_state=42)

#모델 생성 함수
def build_model(dropout_rate, learning_rate):
    model = Sequential([
        Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=(X.shape[1],)),
        BatchNormalization(),
        Dropout(dropout_rate),

        Dense(128, activation='relu', kernel_initializer='he_normal'),
        BatchNormalization(),
        Dropout(dropout_rate),

        Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

#Bayesian Optimization 수행 함수
def model_cv(dropout_rate, learning_rate):
    model = build_model(dropout_rate=dropout_rate, learning_rate=learning_rate)
    history = model.fit(X_sample, y_sample, validation_split=0.1,
                        epochs=30, batch_size=32, verbose=0)
    val_acc = max(history.history['val_accuracy'])
    return val_acc

#탐색 범위 정의 및 최적화 수행
pbounds = {
    'dropout_rate': (0.1, 0.5),
    'learning_rate': (1e-4, 1e-2)
}

optimizer = BayesianOptimization(f=model_cv, pbounds=pbounds, random_state=42)
optimizer.maximize(init_points=5, n_iter=10)

#최적 하이퍼파라미터 출력
print("\n Best Parameters:")
print(optimizer.max)

# **Cross Validation**

knn = KNeighborsClassifier(n_neighbors=22)
knn.fit(X_scaled, y)
#모델 생성
model = Sequential([
    Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=(X.shape[1],)),
    BatchNormalization(),
    Dropout(0.34),

    Dense(128, activation='relu', kernel_initializer='he_normal'),
    BatchNormalization(),
    Dropout(0.34),

    Dense(64, activation='relu', kernel_initializer='he_normal'),
    BatchNormalization(),
    Dropout(0.34),

    Dense(num_classes, activation='softmax')  # 다중 클래스 분류
])
#모델 컴파일 및 학습
model.compile(optimizer=Adam(learning_rate=0.007),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train,
                    validation_split=0.1,
                    epochs=50,
                    batch_size=32,
                    verbose=1)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

#모델 평가
print("장르 분류 정확도 (5-폴드 교차검증):\n")
knn_scores = cross_val_score(knn, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"{'KNN':12s}: 평균 정확도 = {knn_scores.mean():.4f}")

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred_classes)
print("Accuracy:", accuracy)
