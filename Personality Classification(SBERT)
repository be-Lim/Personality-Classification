# **Library**

import os
os.chdir("/content/drive/MyDrive/GRIT")
%load_ext autoreload
%autoreload 2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV
!pip install bayesian-optimization
from bayes_opt import BayesianOptimization
from sklearn.ensemble import VotingClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, accuracy_score

# **EDA**

#데이터 불러오기
df = pd.read_csv('/content/drive/MyDrive/GRIT/MBTI/16P.csv', encoding='ISO-8859-1')
print(df.shape)
print(df.head())

#결측치 확인
print(df.isnull().sum())

mbti_mapping = {
    'ESTJ': 0, 'ENTJ': 1, 'ESFJ': 2, 'ENFJ': 3,
    'ISTJ': 4, 'ISFJ': 5, 'INTJ': 6, 'INFJ': 7,
    'ESTP': 8, 'ESFP': 9, 'ENTP': 10, 'ENFP': 11,
    'ISTP': 12, 'ISFP': 13, 'INTP': 14, 'INFP': 15
}
mbti_inverse_mapping = {v: k for k, v in mbti_mapping.items()}

def encode_mbti_column(df, column_name):
    df[column_name + '_encoded'] = df[column_name].map(mbti_mapping)
    return df
def decode_mbti_column(df, column_name_encoded):
    df[column_name_encoded.replace('_encoded', '') + '_decoded'] = df[column_name_encoded].map(mbti_inverse_mapping)
    return df

#'Personality' 인코딩
df_encoded = encode_mbti_column(df, 'Personality')
df_encoded = df_encoded.drop(['Personality'], axis=1)
df_encoded.head()

# **Sentence Embedding**

df_dropped = df_encoded.drop(['Response Id'], axis=1)
questions = pd.Series(df_dropped.columns[:-1])
print(questions)

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
question_embeddings = model.encode(questions)

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering

similarity_matrix = cosine_similarity(question_embeddings)

clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.3, metric='cosine', linkage='average')
clusters = clustering.fit_predict(question_embeddings)

selected_question_indices = []
seen_clusters = set()

for idx, cluster_id in enumerate(clusters):
    if cluster_id not in seen_clusters:
        selected_question_indices.append(idx)
        seen_clusters.add(cluster_id)

filtered_questions = [questions[i] for i in selected_question_indices]
print(filtered_questions)

features = df_encoded[filtered_questions]
target = df["Personality_encoded"]

selector = SelectKBest(score_func=mutual_info_classif, k=20)
X_selected = selector.fit_transform(features, target)

selected_indices = selector.get_support(indices=True)
final_questions = [filtered_questions[i] for i in selected_indices]
print(final_questions, len(final_questions))

#최종 선택된 데이터
final_features = final_questions + ['Personality_encoded']
df_selected = df_encoded[final_features]
#데이터 분할
X = df_selected.drop('Personality_encoded', axis=1).values
y = df_selected['Personality_encoded'].astype(int).values
#표준 정규화
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled)
print(y)

print(len(X_scaled), len(y))

# **Classification Model - KNN**

#모델 훈련 및 초기 성능 측정
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

knn_scores = cross_val_score(knn, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"KNN: 평균 정확도 = {knn_scores.mean():.4f}, 표준편차 = {knn_scores.std():.4f}")

# 하이퍼파라미터 최적화 함수 정의
def knn_cv(n_neighbors):
    model = KNeighborsClassifier(n_neighbors=int(n_neighbors))
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)

# 파라미터 탐색 범위 설정
param_bounds = {
    'n_neighbors': (1, 30)
}

# 베이지안 최적화 실행
optimizer = BayesianOptimization(
    f=knn_cv,
    pbounds=param_bounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=3, n_iter=10)

# 최적 파라미터 출력
best_params = optimizer.max['params']
best_params['n_neighbors'] = int(best_params['n_neighbors'])

print("\n Best Parameters:")
print(best_params)

#최적화 모델 성능 측정
best_knn = KNeighborsClassifier(n_neighbors=26)
best_knn.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

best_knn_scores = cross_val_score(best_knn, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"best_KNN: 평균 정확도 = {best_knn_scores.mean():.4f}, 표준편차 = {best_knn_scores.std():.4f}")

# **Classification Model - LightGBM**

#모델 훈련 및 초기 성능 측정
lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(np.unique(y)), random_state=42)
lgb_model.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

lgb_scores = cross_val_score(lgb_model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"LightGBM: 평균 정확도 = {knn_scores.mean():.4f}, 표준편차 = {knn_scores.std():.4f}")

# 학습 데이터 샘플링 (속도 향상을 위해)
X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)

# 최적화 대상 함수 정의
def lgb_cv(learning_rate, num_leaves, max_depth, min_child_samples):
    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=len(np.unique(y_sample)),
        random_state=42,
        learning_rate=learning_rate,
        num_leaves=int(num_leaves),
        max_depth=int(max_depth),
        min_child_samples=int(min_child_samples),
        n_estimators=100
    )
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

# 탐색 범위 설정
param_bounds = {
    'learning_rate': (0.01, 0.3),
    'num_leaves': (20, 150),
    'max_depth': (3, 15),
    'min_child_samples': (5, 50)
}

# 베이지안 최적화 실행
optimizer = BayesianOptimization(
    f=lgb_cv,
    pbounds=param_bounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=5, n_iter=15)

# 최적 하이퍼파라미터 출력
best_params = optimizer.max['params']
best_params['num_leaves'] = int(best_params['num_leaves'])
best_params['max_depth'] = int(best_params['max_depth'])
best_params['min_child_samples'] = int(best_params['min_child_samples'])

print("\nBest Parameters for LightGBM:")
print(best_params)

#최적화 모델 성능 측정
import seaborn as sns
best_lgb_model = lgb.LGBMClassifier(objective='multiclass',
                                    num_class=len(np.unique(y_sample)),
                                    random_state=42,
                                    learning_rate=float(0.3),
                                    num_leaves=int(146),
                                    max_depth=int(7),
                                    min_child_samples=int(27),
                                    n_estimators=100)
best_lgb_model.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

best_lgb_scores = cross_val_score(best_lgb_model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1)
print(f"best_LightGBM: 평균 정확도 = {best_lgb_scores.mean():.4f}, 표준편차 = {best_lgb_scores.std():.4f}")

# **Classification Model - XGBoost**

#모델 훈련 및 초기 성능 측정
X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)

xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    n_estimators=300,
    learning_rate=0.1,
    max_depth=6,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)
xgb_model.fit(X_sample, y_sample)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

xgb_scores = cross_val_score(xgb_model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"XGBoost: 평균 정확도 = {knn_scores.mean():.4f}, 표준편차 = {knn_scores.std():.4f}")

# 데이터 샘플링
X_sample, _, y_sample, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)

# 최적화 대상 함수 정의
def xgb_cv(learning_rate, max_depth, min_child_weight, subsample, colsample_bytree):
    model = XGBClassifier(
        objective='multi:softmax',
        num_class=len(np.unique(y_sample)),
        use_label_encoder=False,
        eval_metric='mlogloss',
        learning_rate=learning_rate,
        max_depth=int(max_depth),
        min_child_weight=int(min_child_weight),
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        n_estimators=100,
        random_state=42,
        verbosity=0
    )
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

param_bounds = {
    'learning_rate': (0.01, 0.3),
    'max_depth': (3, 15),
    'min_child_weight': (1, 10),
    'subsample': (0.5, 1.0),
    'colsample_bytree': (0.5, 1.0)
}

optimizer = BayesianOptimization(
    f=xgb_cv,
    pbounds=param_bounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=5, n_iter=15)

# 최적 파라미터 추출
best_params = optimizer.max['params']
best_params['max_depth'] = int(best_params['max_depth'])
best_params['min_child_weight'] = int(best_params['min_child_weight'])

print("\nBest Parameters for XGBoost:")
print(best_params)

#최적화 모델 성능 측정
best_xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    n_estimators=300,
    learning_rate=0.18,
    max_depth=14,
    min_child_weight=7,
    subsample=0.92,
    colsample_bytree=0.57,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)
best_xgb_model.fit(X_scaled, y)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

best_xgb_scores = cross_val_score(best_xgb_model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
print(f"best_XGBoost: 평균 정확도 = {best_xgb_scores.mean():.4f}, 표준편차 = {best_xgb_scores.std():.4f}")

# **Classification Model - ANN**

# 클래스 수 확인 및 원핫 인코딩
num_classes = len(np.unique(y))
y_encoded = to_categorical(y, num_classes)
# 학습/테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, shuffle=True, random_state=42)

#모델 생성
ann_model = Sequential([
    Dense(256, activation='relu', input_shape=(X.shape[1],)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),

    Dense(num_classes, activation='softmax')
])

#모델 컴파일 및 학습
ann_model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

ann_model.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=64, verbose=1)

#모델 평가
y_pred = ann_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred_classes)
print("Accuracy:", accuracy)

# **Model Tuning - ANN**

X_sample, _, y_sample, _ = train_test_split(X_scaled, y_encoded, train_size=0.2, random_state=42)

#모델 생성 함수
def build_model(dropout_rate, learning_rate):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(X.shape[1],)),
        BatchNormalization(),
        Dropout(dropout_rate),

        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),

        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),

        Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

#Bayesian Optimization 수행 함수
def model_cv(dropout_rate, learning_rate):
    model = build_model(dropout_rate=dropout_rate, learning_rate=learning_rate)
    history = model.fit(X_sample, y_sample, validation_split=0.1,
                        epochs=30, batch_size=64, verbose=0)
    val_acc = max(history.history['val_accuracy'])
    return val_acc

#탐색 범위 정의 및 최적화 수행
pbounds = {
    'dropout_rate': (0.1, 0.5),
    'learning_rate': (1e-4, 1e-2)
}

optimizer = BayesianOptimization(f=model_cv, pbounds=pbounds, random_state=42)
optimizer.maximize(init_points=5, n_iter=10)

#최적 하이퍼파라미터 출력
print("\n Best Parameters:")
print(optimizer.max)

#모델 생성
def best_ann_model(input_dim, num_classes):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,)),
        BatchNormalization(),
        Dropout(0.16),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.16),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=0.0015),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 학습
best_ann = best_ann_model(X_train.shape[1], num_classes)
best_ann.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=64, verbose=1)

#최적화 모델 평가
y_pred = best_ann.predict(X_scaled)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_encoded, axis=1)
accuracy = accuracy_score(y_true, y_pred_classes)
print("best_Accuracy:", accuracy)

# **Ensemble(KNN+ANN)**

def ensemble_model(knn, ann, X):
  knn_probs = knn.predict_proba(X)
  ann_probs = ann.predict(X)

  avg_probs =(knn_probs + ann_probs) / 2

  return avg_probs

y_pred_classes = np.argmax(ensemble_model(best_knn, best_ann, X_scaled), axis=1)
y_true = np.argmax(y_encoded, axis=1)
ensemble_accuracy = accuracy_score(y_true, y_pred_classes)
print("best_Accuracy:", ensemble_accuracy)

for i, prob in enumerate(ensemble_model(best_knn, best_ann, X_scaled)[0]):
    print(f"{mbti_inverse_mapping[i]}: {prob:.4f}")

# **Test**

#(-3 ~ 3) = (전혀 그렇지 않다 ~ 매우 그렇다)
from google.colab import files
questions_with_blank_answers = np.column_stack((final_questions, [''] * len(final_questions)))
data = pd.DataFrame(questions_with_blank_answers, columns=["question", "answer"])
data.to_csv("/content/drive/MyDrive/GRIT/MBTI/test_question.csv", index=False)
files.download("/content/drive/MyDrive/GRIT/MBTI/test_question.csv")

test_answer = pd.read_csv('/content/drive/MyDrive/GRIT/MBTI/test_answer_2.csv', encoding='ISO-8859-1')
data = test_answer.iloc[:, 1]
data = data.values.reshape(1, -1)
print(data)

data = np.array([[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]])
data_scaled = scaler.fit_transform(data.T)
print(data_scaled.T)
input_data = data_scaled.T
prediction = ensemble_model(best_knn, best_ann, input_data)
prediction_classes = np.argmax(prediction, axis=1)
print(prediction_classes)
print("Predicted_MBTI:", mbti_inverse_mapping[prediction_classes[0]])
for i, prob in enumerate(prediction[0]):
    print(f"{mbti_inverse_mapping[i]}: {prob:.4f}")

